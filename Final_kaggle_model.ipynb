{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e2d1914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from catboost import CatBoostClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bab4bf7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>N_Days</th>\n",
       "      <th>Drug</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Ascites</th>\n",
       "      <th>Hepatomegaly</th>\n",
       "      <th>Spiders</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Bilirubin</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>Albumin</th>\n",
       "      <th>Copper</th>\n",
       "      <th>Alk_Phos</th>\n",
       "      <th>SGOT</th>\n",
       "      <th>Tryglicerides</th>\n",
       "      <th>Platelets</th>\n",
       "      <th>Prothrombin</th>\n",
       "      <th>Stage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15000</td>\n",
       "      <td>130.0</td>\n",
       "      <td>D-penicillamine</td>\n",
       "      <td>16944.0</td>\n",
       "      <td>F</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>17.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.00</td>\n",
       "      <td>182.0</td>\n",
       "      <td>559.0</td>\n",
       "      <td>119.35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>401.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15001</td>\n",
       "      <td>2574.0</td>\n",
       "      <td>D-penicillamine</td>\n",
       "      <td>17664.0</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>0.9</td>\n",
       "      <td>242.0</td>\n",
       "      <td>3.65</td>\n",
       "      <td>108.0</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>108.50</td>\n",
       "      <td>118.0</td>\n",
       "      <td>344.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15002</td>\n",
       "      <td>3853.0</td>\n",
       "      <td>Placebo</td>\n",
       "      <td>13736.0</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>1.6</td>\n",
       "      <td>354.0</td>\n",
       "      <td>3.80</td>\n",
       "      <td>44.0</td>\n",
       "      <td>1584.0</td>\n",
       "      <td>111.60</td>\n",
       "      <td>108.0</td>\n",
       "      <td>277.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15003</td>\n",
       "      <td>2249.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23011.0</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>0.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>190.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15004</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17046.0</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>0.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.66</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>350.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>24995</td>\n",
       "      <td>2713.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17532.0</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>1.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>330.0</td>\n",
       "      <td>9.9</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>24996</td>\n",
       "      <td>2580.0</td>\n",
       "      <td>D-penicillamine</td>\n",
       "      <td>25569.0</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>0.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.01</td>\n",
       "      <td>20.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>54.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>277.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>24997</td>\n",
       "      <td>186.0</td>\n",
       "      <td>Placebo</td>\n",
       "      <td>21483.0</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>S</td>\n",
       "      <td>6.6</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>3.50</td>\n",
       "      <td>188.0</td>\n",
       "      <td>944.0</td>\n",
       "      <td>130.20</td>\n",
       "      <td>133.0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>24998</td>\n",
       "      <td>2221.0</td>\n",
       "      <td>Placebo</td>\n",
       "      <td>16728.0</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>0.9</td>\n",
       "      <td>434.0</td>\n",
       "      <td>3.36</td>\n",
       "      <td>161.0</td>\n",
       "      <td>1523.0</td>\n",
       "      <td>117.80</td>\n",
       "      <td>166.0</td>\n",
       "      <td>381.0</td>\n",
       "      <td>9.9</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>24999</td>\n",
       "      <td>778.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23376.0</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>2.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>190.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  N_Days             Drug      Age Sex Ascites Hepatomegaly  \\\n",
       "0     15000   130.0  D-penicillamine  16944.0   F       Y            Y   \n",
       "1     15001  2574.0  D-penicillamine  17664.0   F       N            Y   \n",
       "2     15002  3853.0          Placebo  13736.0   F       N            N   \n",
       "3     15003  2249.0              NaN  23011.0   F     NaN          NaN   \n",
       "4     15004  1150.0              NaN  17046.0   F     NaN          NaN   \n",
       "...     ...     ...              ...      ...  ..     ...          ...   \n",
       "9995  24995  2713.0              NaN  17532.0   F     NaN          NaN   \n",
       "9996  24996  2580.0  D-penicillamine  25569.0   F       N            N   \n",
       "9997  24997   186.0          Placebo  21483.0   F       N            Y   \n",
       "9998  24998  2221.0          Placebo  16728.0   F       N            Y   \n",
       "9999  24999   778.0              NaN  23376.0   F     NaN          NaN   \n",
       "\n",
       "     Spiders Edema  Bilirubin  Cholesterol  Albumin  Copper  Alk_Phos    SGOT  \\\n",
       "0          Y     N       17.4          NaN     3.00   182.0     559.0  119.35   \n",
       "1          N     N        0.9        242.0     3.65   108.0    1040.0  108.50   \n",
       "2          Y     N        1.6        354.0     3.80    44.0    1584.0  111.60   \n",
       "3        NaN     N        0.9          NaN     3.06     NaN       NaN     NaN   \n",
       "4        NaN     N        0.7          NaN     3.66     NaN       NaN     NaN   \n",
       "...      ...   ...        ...          ...      ...     ...       ...     ...   \n",
       "9995     NaN     N        1.1          NaN     3.75     NaN       NaN     NaN   \n",
       "9996       N     N        0.4          NaN     4.01    20.0     666.0   54.25   \n",
       "9997       Y     S        6.6       1000.0     3.50   188.0     944.0  130.20   \n",
       "9998       N     N        0.9        434.0     3.36   161.0    1523.0  117.80   \n",
       "9999     NaN     S        2.3          NaN     3.14     NaN       NaN     NaN   \n",
       "\n",
       "      Tryglicerides  Platelets  Prothrombin  Stage  \n",
       "0               NaN      401.0         11.0    4.0  \n",
       "1             118.0      344.0         11.0    3.0  \n",
       "2             108.0      277.0         10.3    4.0  \n",
       "3               NaN      190.0         11.5    4.0  \n",
       "4               NaN      350.0         10.3    2.0  \n",
       "...             ...        ...          ...    ...  \n",
       "9995            NaN      330.0          9.9    3.0  \n",
       "9996            NaN      277.0         10.0    3.0  \n",
       "9997          133.0      265.0         11.0    4.0  \n",
       "9998          166.0      381.0          9.9    2.0  \n",
       "9999            NaN      190.0         11.5    4.0  \n",
       "\n",
       "[10000 rows x 19 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !conda install imbalanced-learn\n",
    "import pickle\n",
    "\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "25f77929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subsample': 0.8, 'reg_lambda': 1, 'reg_alpha': 1, 'n_estimators': 300, 'min_child_weight': 1, 'max_depth': 3, 'learning_rate': 0.1, 'gamma': 1, 'colsample_bytree': 0.8}\n",
      "0:\tlearn: 1.0274453\ttotal: 87.4ms\tremaining: 1m 27s\n",
      "200:\tlearn: 0.2422674\ttotal: 3.55s\tremaining: 14.1s\n",
      "400:\tlearn: 0.2182612\ttotal: 6.86s\tremaining: 10.3s\n",
      "600:\tlearn: 0.2013990\ttotal: 10.6s\tremaining: 7.04s\n",
      "800:\tlearn: 0.1873033\ttotal: 14.5s\tremaining: 3.6s\n",
      "999:\tlearn: 0.1749912\ttotal: 17.6s\tremaining: 0us\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004590 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2318\n",
      "[LightGBM] [Info] Number of data points in the train set: 17607, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -0.362444\n",
      "[LightGBM] [Info] Start training from score -4.123563\n",
      "[LightGBM] [Info] Start training from score -1.245350\n",
      "0:\tlearn: 1.0252476\ttotal: 14ms\tremaining: 13.9s\n",
      "200:\tlearn: 0.2155422\ttotal: 3.09s\tremaining: 12.3s\n",
      "400:\tlearn: 0.1919729\ttotal: 6.18s\tremaining: 9.24s\n",
      "600:\tlearn: 0.1748786\ttotal: 9.85s\tremaining: 6.54s\n",
      "800:\tlearn: 0.1600789\ttotal: 13s\tremaining: 3.22s\n",
      "999:\tlearn: 0.1472406\ttotal: 16.2s\tremaining: 0us\n",
      "0:\tlearn: 1.0253020\ttotal: 47.8ms\tremaining: 47.8s\n",
      "200:\tlearn: 0.2172247\ttotal: 3.97s\tremaining: 15.8s\n",
      "400:\tlearn: 0.1941242\ttotal: 7.45s\tremaining: 11.1s\n",
      "600:\tlearn: 0.1767276\ttotal: 10.7s\tremaining: 7.1s\n",
      "800:\tlearn: 0.1621033\ttotal: 15.1s\tremaining: 3.76s\n",
      "999:\tlearn: 0.1499263\ttotal: 19.6s\tremaining: 0us\n",
      "0:\tlearn: 1.0253941\ttotal: 11.1ms\tremaining: 11.1s\n",
      "200:\tlearn: 0.2147850\ttotal: 3.11s\tremaining: 12.4s\n",
      "400:\tlearn: 0.1911654\ttotal: 6.02s\tremaining: 9s\n",
      "600:\tlearn: 0.1719314\ttotal: 9.2s\tremaining: 6.11s\n",
      "800:\tlearn: 0.1566467\ttotal: 12.2s\tremaining: 3.03s\n",
      "999:\tlearn: 0.1445591\ttotal: 16s\tremaining: 0us\n",
      "0:\tlearn: 1.0306405\ttotal: 16.1ms\tremaining: 16.1s\n",
      "200:\tlearn: 0.2703720\ttotal: 3.69s\tremaining: 14.7s\n",
      "400:\tlearn: 0.2426068\ttotal: 7.44s\tremaining: 11.1s\n",
      "600:\tlearn: 0.2216379\ttotal: 11.1s\tremaining: 7.34s\n",
      "800:\tlearn: 0.2045592\ttotal: 15.3s\tremaining: 3.79s\n",
      "999:\tlearn: 0.1896069\ttotal: 19.4s\tremaining: 0us\n",
      "0:\tlearn: 1.0316183\ttotal: 14.3ms\tremaining: 14.3s\n",
      "200:\tlearn: 0.2777520\ttotal: 3.45s\tremaining: 13.7s\n",
      "400:\tlearn: 0.2490069\ttotal: 6.99s\tremaining: 10.4s\n",
      "600:\tlearn: 0.2282525\ttotal: 11s\tremaining: 7.27s\n",
      "800:\tlearn: 0.2111741\ttotal: 14.6s\tremaining: 3.63s\n",
      "999:\tlearn: 0.1967127\ttotal: 18.5s\tremaining: 0us\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001043 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2312\n",
      "[LightGBM] [Info] Number of data points in the train set: 14085, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -0.362422\n",
      "[LightGBM] [Info] Start training from score -4.123520\n",
      "[LightGBM] [Info] Start training from score -1.245406\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001584 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2317\n",
      "[LightGBM] [Info] Number of data points in the train set: 14085, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -0.362422\n",
      "[LightGBM] [Info] Start training from score -4.123520\n",
      "[LightGBM] [Info] Start training from score -1.245406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001792 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2313\n",
      "[LightGBM] [Info] Number of data points in the train set: 14086, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -0.362391\n",
      "[LightGBM] [Info] Start training from score -4.123591\n",
      "[LightGBM] [Info] Start training from score -1.245477\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000959 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2309\n",
      "[LightGBM] [Info] Number of data points in the train set: 14086, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -0.362493\n",
      "[LightGBM] [Info] Start training from score -4.123591\n",
      "[LightGBM] [Info] Start training from score -1.245231\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004236 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2312\n",
      "[LightGBM] [Info] Number of data points in the train set: 14086, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -0.362493\n",
      "[LightGBM] [Info] Start training from score -4.123591\n",
      "[LightGBM] [Info] Start training from score -1.245231\n",
      "0:\tlearn: 1.0276269\ttotal: 10.3ms\tremaining: 10.3s\n",
      "200:\tlearn: 0.2477358\ttotal: 3.25s\tremaining: 12.9s\n",
      "400:\tlearn: 0.2242579\ttotal: 6.81s\tremaining: 10.2s\n",
      "600:\tlearn: 0.2071281\ttotal: 10.9s\tremaining: 7.25s\n",
      "800:\tlearn: 0.1921153\ttotal: 14.2s\tremaining: 3.52s\n",
      "999:\tlearn: 0.1798749\ttotal: 17.6s\tremaining: 0us\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001251 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2325\n",
      "[LightGBM] [Info] Number of data points in the train set: 17607, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -0.362444\n",
      "[LightGBM] [Info] Start training from score -4.123563\n",
      "[LightGBM] [Info] Start training from score -1.245350\n",
      "0:\tlearn: 1.0251139\ttotal: 11.9ms\tremaining: 11.8s\n",
      "200:\tlearn: 0.2142731\ttotal: 3.11s\tremaining: 12.4s\n",
      "400:\tlearn: 0.1904319\ttotal: 6.14s\tremaining: 9.18s\n",
      "600:\tlearn: 0.1728045\ttotal: 9.4s\tremaining: 6.24s\n",
      "800:\tlearn: 0.1587434\ttotal: 12.5s\tremaining: 3.12s\n",
      "999:\tlearn: 0.1459912\ttotal: 15.6s\tremaining: 0us\n",
      "0:\tlearn: 1.0261839\ttotal: 10.9ms\tremaining: 10.9s\n",
      "200:\tlearn: 0.2247048\ttotal: 3.28s\tremaining: 13s\n",
      "400:\tlearn: 0.2012231\ttotal: 6.36s\tremaining: 9.5s\n",
      "600:\tlearn: 0.1834117\ttotal: 9.41s\tremaining: 6.25s\n",
      "800:\tlearn: 0.1680688\ttotal: 12.4s\tremaining: 3.09s\n",
      "999:\tlearn: 0.1552566\ttotal: 15.4s\tremaining: 0us\n",
      "0:\tlearn: 1.0261104\ttotal: 8.8ms\tremaining: 8.79s\n",
      "200:\tlearn: 0.2203085\ttotal: 3.2s\tremaining: 12.7s\n",
      "400:\tlearn: 0.1963755\ttotal: 6.26s\tremaining: 9.35s\n",
      "600:\tlearn: 0.1780541\ttotal: 9.36s\tremaining: 6.21s\n",
      "800:\tlearn: 0.1630158\ttotal: 12.5s\tremaining: 3.1s\n",
      "999:\tlearn: 0.1495981\ttotal: 15.8s\tremaining: 0us\n",
      "0:\tlearn: 1.0313071\ttotal: 9.77ms\tremaining: 9.76s\n",
      "200:\tlearn: 0.2781989\ttotal: 3.12s\tremaining: 12.4s\n",
      "400:\tlearn: 0.2495968\ttotal: 6.29s\tremaining: 9.4s\n",
      "600:\tlearn: 0.2303203\ttotal: 9.41s\tremaining: 6.25s\n",
      "800:\tlearn: 0.2130441\ttotal: 12.5s\tremaining: 3.11s\n",
      "999:\tlearn: 0.1979291\ttotal: 15.6s\tremaining: 0us\n",
      "0:\tlearn: 1.0321534\ttotal: 12ms\tremaining: 12s\n",
      "200:\tlearn: 0.2825734\ttotal: 3.13s\tremaining: 12.4s\n",
      "400:\tlearn: 0.2556805\ttotal: 6.22s\tremaining: 9.29s\n",
      "600:\tlearn: 0.2348975\ttotal: 9.24s\tremaining: 6.14s\n",
      "800:\tlearn: 0.2175577\ttotal: 12.3s\tremaining: 3.05s\n",
      "999:\tlearn: 0.2019166\ttotal: 15.3s\tremaining: 0us\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002199 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2316\n",
      "[LightGBM] [Info] Number of data points in the train set: 14085, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -0.362422\n",
      "[LightGBM] [Info] Start training from score -4.123520\n",
      "[LightGBM] [Info] Start training from score -1.245406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000834 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2314\n",
      "[LightGBM] [Info] Number of data points in the train set: 14085, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -0.362422\n",
      "[LightGBM] [Info] Start training from score -4.123520\n",
      "[LightGBM] [Info] Start training from score -1.245406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000979 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2319\n",
      "[LightGBM] [Info] Number of data points in the train set: 14086, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -0.362493\n",
      "[LightGBM] [Info] Start training from score -4.123591\n",
      "[LightGBM] [Info] Start training from score -1.245231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000981 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2315\n",
      "[LightGBM] [Info] Number of data points in the train set: 14086, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -0.362493\n",
      "[LightGBM] [Info] Start training from score -4.123591\n",
      "[LightGBM] [Info] Start training from score -1.245231\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000904 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2314\n",
      "[LightGBM] [Info] Number of data points in the train set: 14086, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -0.362391\n",
      "[LightGBM] [Info] Start training from score -4.123591\n",
      "[LightGBM] [Info] Start training from score -1.245477\n",
      "0:\tlearn: 1.0277690\ttotal: 26.3ms\tremaining: 26.3s\n",
      "200:\tlearn: 0.2469893\ttotal: 3.31s\tremaining: 13.2s\n",
      "400:\tlearn: 0.2232241\ttotal: 6.75s\tremaining: 10.1s\n",
      "600:\tlearn: 0.2051184\ttotal: 10.1s\tremaining: 6.69s\n",
      "800:\tlearn: 0.1906699\ttotal: 13.3s\tremaining: 3.31s\n",
      "999:\tlearn: 0.1778518\ttotal: 16.7s\tremaining: 0us\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001523 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2315\n",
      "[LightGBM] [Info] Number of data points in the train set: 17607, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -0.362444\n",
      "[LightGBM] [Info] Start training from score -4.123563\n",
      "[LightGBM] [Info] Start training from score -1.245350\n",
      "0:\tlearn: 1.0248371\ttotal: 66.8ms\tremaining: 1m 6s\n",
      "200:\tlearn: 0.2146675\ttotal: 3.05s\tremaining: 12.1s\n",
      "400:\tlearn: 0.1898163\ttotal: 6.05s\tremaining: 9.04s\n",
      "600:\tlearn: 0.1717959\ttotal: 9.11s\tremaining: 6.05s\n",
      "800:\tlearn: 0.1566050\ttotal: 12.2s\tremaining: 3.02s\n",
      "999:\tlearn: 0.1441151\ttotal: 15.2s\tremaining: 0us\n",
      "0:\tlearn: 1.0257241\ttotal: 14.8ms\tremaining: 14.8s\n",
      "200:\tlearn: 0.2197341\ttotal: 3.08s\tremaining: 12.3s\n",
      "400:\tlearn: 0.1948540\ttotal: 6.11s\tremaining: 9.12s\n",
      "600:\tlearn: 0.1778107\ttotal: 9s\tremaining: 5.97s\n",
      "800:\tlearn: 0.1629372\ttotal: 12s\tremaining: 2.97s\n",
      "999:\tlearn: 0.1497560\ttotal: 14.8s\tremaining: 0us\n",
      "0:\tlearn: 1.0263461\ttotal: 13.6ms\tremaining: 13.6s\n",
      "200:\tlearn: 0.2252890\ttotal: 3.06s\tremaining: 12.2s\n",
      "400:\tlearn: 0.2000947\ttotal: 6.05s\tremaining: 9.04s\n",
      "600:\tlearn: 0.1818318\ttotal: 9.06s\tremaining: 6.02s\n",
      "800:\tlearn: 0.1670071\ttotal: 12s\tremaining: 2.99s\n",
      "999:\tlearn: 0.1537862\ttotal: 15s\tremaining: 0us\n",
      "0:\tlearn: 1.0311121\ttotal: 18.3ms\tremaining: 18.3s\n",
      "200:\tlearn: 0.2757385\ttotal: 3.02s\tremaining: 12s\n",
      "400:\tlearn: 0.2472486\ttotal: 6s\tremaining: 8.97s\n",
      "600:\tlearn: 0.2268811\ttotal: 9.07s\tremaining: 6.02s\n",
      "800:\tlearn: 0.2093078\ttotal: 12.3s\tremaining: 3.05s\n",
      "999:\tlearn: 0.1943530\ttotal: 15.4s\tremaining: 0us\n",
      "0:\tlearn: 1.0320288\ttotal: 9.28ms\tremaining: 9.27s\n",
      "200:\tlearn: 0.2815578\ttotal: 3.07s\tremaining: 12.2s\n",
      "400:\tlearn: 0.2525845\ttotal: 6.11s\tremaining: 9.12s\n",
      "600:\tlearn: 0.2311204\ttotal: 9.37s\tremaining: 6.22s\n",
      "800:\tlearn: 0.2135104\ttotal: 12.4s\tremaining: 3.08s\n",
      "999:\tlearn: 0.1982047\ttotal: 15.5s\tremaining: 0us\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001262 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2308\n",
      "[LightGBM] [Info] Number of data points in the train set: 14085, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -0.362422\n",
      "[LightGBM] [Info] Start training from score -4.123520\n",
      "[LightGBM] [Info] Start training from score -1.245406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004917 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2308\n",
      "[LightGBM] [Info] Number of data points in the train set: 14085, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -0.362422\n",
      "[LightGBM] [Info] Start training from score -4.123520\n",
      "[LightGBM] [Info] Start training from score -1.245406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000906 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2314\n",
      "[LightGBM] [Info] Number of data points in the train set: 14086, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -0.362493\n",
      "[LightGBM] [Info] Start training from score -4.123591\n",
      "[LightGBM] [Info] Start training from score -1.245231\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000871 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2312\n",
      "[LightGBM] [Info] Number of data points in the train set: 14086, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -0.362493\n",
      "[LightGBM] [Info] Start training from score -4.123591\n",
      "[LightGBM] [Info] Start training from score -1.245231\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000794 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2313\n",
      "[LightGBM] [Info] Number of data points in the train set: 14086, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -0.362391\n",
      "[LightGBM] [Info] Start training from score -4.123591\n",
      "[LightGBM] [Info] Start training from score -1.245477\n",
      "0:\tlearn: 1.0313583\ttotal: 14.4ms\tremaining: 14.4s\n",
      "200:\tlearn: 0.2808109\ttotal: 3.31s\tremaining: 13.2s\n",
      "400:\tlearn: 0.2550011\ttotal: 6.47s\tremaining: 9.67s\n",
      "600:\tlearn: 0.2362081\ttotal: 9.72s\tremaining: 6.46s\n",
      "800:\tlearn: 0.2201670\ttotal: 13.4s\tremaining: 3.33s\n",
      "999:\tlearn: 0.2066961\ttotal: 16.7s\tremaining: 0us\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001434 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2320\n",
      "[LightGBM] [Info] Number of data points in the train set: 17607, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -0.362363\n",
      "[LightGBM] [Info] Start training from score -4.127078\n",
      "[LightGBM] [Info] Start training from score -1.245350\n",
      "0:\tlearn: 1.0294831\ttotal: 12.1ms\tremaining: 12.1s\n",
      "200:\tlearn: 0.2572232\ttotal: 3.21s\tremaining: 12.7s\n",
      "400:\tlearn: 0.2296620\ttotal: 6.26s\tremaining: 9.35s\n",
      "600:\tlearn: 0.2091925\ttotal: 9.28s\tremaining: 6.16s\n",
      "800:\tlearn: 0.1919698\ttotal: 12.3s\tremaining: 3.06s\n",
      "999:\tlearn: 0.1780367\ttotal: 15.3s\tremaining: 0us\n",
      "0:\tlearn: 1.0300092\ttotal: 13.3ms\tremaining: 13.3s\n",
      "200:\tlearn: 0.2641931\ttotal: 3.07s\tremaining: 12.2s\n",
      "400:\tlearn: 0.2381198\ttotal: 6.15s\tremaining: 9.18s\n",
      "600:\tlearn: 0.2175783\ttotal: 9.13s\tremaining: 6.06s\n",
      "800:\tlearn: 0.2003244\ttotal: 12.2s\tremaining: 3.02s\n",
      "999:\tlearn: 0.1853998\ttotal: 15.2s\tremaining: 0us\n",
      "0:\tlearn: 1.0302864\ttotal: 10.5ms\tremaining: 10.5s\n",
      "200:\tlearn: 0.2665005\ttotal: 3.06s\tremaining: 12.2s\n",
      "400:\tlearn: 0.2402731\ttotal: 6.17s\tremaining: 9.22s\n",
      "600:\tlearn: 0.2207828\ttotal: 10.1s\tremaining: 6.74s\n",
      "800:\tlearn: 0.2040302\ttotal: 14.1s\tremaining: 3.5s\n",
      "999:\tlearn: 0.1889003\ttotal: 17.7s\tremaining: 0us\n",
      "0:\tlearn: 1.0325011\ttotal: 24.7ms\tremaining: 24.7s\n",
      "200:\tlearn: 0.2774388\ttotal: 3.14s\tremaining: 12.5s\n",
      "400:\tlearn: 0.2496984\ttotal: 6.25s\tremaining: 9.34s\n",
      "600:\tlearn: 0.2285555\ttotal: 9.19s\tremaining: 6.1s\n",
      "800:\tlearn: 0.2109681\ttotal: 12.2s\tremaining: 3.03s\n",
      "999:\tlearn: 0.1957400\ttotal: 15.2s\tremaining: 0us\n",
      "0:\tlearn: 1.0360329\ttotal: 10.8ms\tremaining: 10.8s\n",
      "200:\tlearn: 0.3209010\ttotal: 3.11s\tremaining: 12.4s\n",
      "400:\tlearn: 0.2917976\ttotal: 6.17s\tremaining: 9.22s\n",
      "600:\tlearn: 0.2686801\ttotal: 9.35s\tremaining: 6.2s\n",
      "800:\tlearn: 0.2486540\ttotal: 12.4s\tremaining: 3.08s\n",
      "999:\tlearn: 0.2319872\ttotal: 15.5s\tremaining: 0us\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008607 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2319\n",
      "[LightGBM] [Info] Number of data points in the train set: 14085, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -0.362320\n",
      "[LightGBM] [Info] Start training from score -4.127916\n",
      "[LightGBM] [Info] Start training from score -1.245406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000978 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2314\n",
      "[LightGBM] [Info] Number of data points in the train set: 14085, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -0.362320\n",
      "[LightGBM] [Info] Start training from score -4.127916\n",
      "[LightGBM] [Info] Start training from score -1.245406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000863 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2316\n",
      "[LightGBM] [Info] Number of data points in the train set: 14086, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -0.362391\n",
      "[LightGBM] [Info] Start training from score -4.123591\n",
      "[LightGBM] [Info] Start training from score -1.245477\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003809 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2311\n",
      "[LightGBM] [Info] Number of data points in the train set: 14086, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -0.362391\n",
      "[LightGBM] [Info] Start training from score -4.127987\n",
      "[LightGBM] [Info] Start training from score -1.245231\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001238 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2317\n",
      "[LightGBM] [Info] Number of data points in the train set: 14086, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -0.362391\n",
      "[LightGBM] [Info] Start training from score -4.127987\n",
      "[LightGBM] [Info] Start training from score -1.245231\n",
      "0:\tlearn: 1.0340089\ttotal: 14.4ms\tremaining: 14.4s\n",
      "200:\tlearn: 0.3063576\ttotal: 3.3s\tremaining: 13.1s\n",
      "400:\tlearn: 0.2788275\ttotal: 6.58s\tremaining: 9.83s\n",
      "600:\tlearn: 0.2585538\ttotal: 9.83s\tremaining: 6.53s\n",
      "800:\tlearn: 0.2423294\ttotal: 13s\tremaining: 3.22s\n",
      "999:\tlearn: 0.2279014\ttotal: 16.2s\tremaining: 0us\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001565 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2318\n",
      "[LightGBM] [Info] Number of data points in the train set: 17608, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -0.362419\n",
      "[LightGBM] [Info] Start training from score -4.123619\n",
      "[LightGBM] [Info] Start training from score -1.245407\n",
      "0:\tlearn: 1.0328104\ttotal: 16.5ms\tremaining: 16.5s\n",
      "200:\tlearn: 0.2908790\ttotal: 3.11s\tremaining: 12.4s\n",
      "400:\tlearn: 0.2628239\ttotal: 6.11s\tremaining: 9.12s\n",
      "600:\tlearn: 0.2407650\ttotal: 9.03s\tremaining: 6s\n",
      "800:\tlearn: 0.2230329\ttotal: 12.1s\tremaining: 2.99s\n",
      "999:\tlearn: 0.2072330\ttotal: 15.1s\tremaining: 0us\n",
      "0:\tlearn: 1.0337027\ttotal: 10.2ms\tremaining: 10.2s\n",
      "200:\tlearn: 0.2959340\ttotal: 2.97s\tremaining: 11.8s\n",
      "400:\tlearn: 0.2676758\ttotal: 6.03s\tremaining: 9.01s\n",
      "600:\tlearn: 0.2448512\ttotal: 9.03s\tremaining: 5.99s\n",
      "800:\tlearn: 0.2269850\ttotal: 11.9s\tremaining: 2.96s\n",
      "999:\tlearn: 0.2109547\ttotal: 14.9s\tremaining: 0us\n",
      "0:\tlearn: 1.0333221\ttotal: 15.4ms\tremaining: 15.4s\n",
      "200:\tlearn: 0.2976786\ttotal: 3.02s\tremaining: 12s\n",
      "400:\tlearn: 0.2686813\ttotal: 6s\tremaining: 8.97s\n",
      "600:\tlearn: 0.2468847\ttotal: 9.05s\tremaining: 6.01s\n",
      "800:\tlearn: 0.2293508\ttotal: 12s\tremaining: 2.99s\n",
      "999:\tlearn: 0.2144147\ttotal: 15s\tremaining: 0us\n",
      "0:\tlearn: 1.0335084\ttotal: 11.3ms\tremaining: 11.3s\n",
      "200:\tlearn: 0.2969892\ttotal: 3.11s\tremaining: 12.3s\n",
      "400:\tlearn: 0.2674299\ttotal: 6.11s\tremaining: 9.12s\n",
      "600:\tlearn: 0.2453731\ttotal: 8.96s\tremaining: 5.95s\n",
      "800:\tlearn: 0.2264378\ttotal: 12s\tremaining: 2.98s\n",
      "999:\tlearn: 0.2108223\ttotal: 15s\tremaining: 0us\n",
      "0:\tlearn: 1.0372333\ttotal: 11.1ms\tremaining: 11.1s\n",
      "200:\tlearn: 0.3348428\ttotal: 3.07s\tremaining: 12.2s\n",
      "400:\tlearn: 0.3041830\ttotal: 6.15s\tremaining: 9.18s\n",
      "600:\tlearn: 0.2808276\ttotal: 9.2s\tremaining: 6.11s\n",
      "800:\tlearn: 0.2618438\ttotal: 12.3s\tremaining: 3.06s\n",
      "999:\tlearn: 0.2453509\ttotal: 15.4s\tremaining: 0us\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001169 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2311\n",
      "[LightGBM] [Info] Number of data points in the train set: 14086, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -0.362391\n",
      "[LightGBM] [Info] Start training from score -4.123591\n",
      "[LightGBM] [Info] Start training from score -1.245477\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007747 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2311\n",
      "[LightGBM] [Info] Number of data points in the train set: 14086, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -0.362391\n",
      "[LightGBM] [Info] Start training from score -4.123591\n",
      "[LightGBM] [Info] Start training from score -1.245477\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000632 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2314\n",
      "[LightGBM] [Info] Number of data points in the train set: 14086, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -0.362391\n",
      "[LightGBM] [Info] Start training from score -4.123591\n",
      "[LightGBM] [Info] Start training from score -1.245477\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000936 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2311\n",
      "[LightGBM] [Info] Number of data points in the train set: 14087, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -0.362462\n",
      "[LightGBM] [Info] Start training from score -4.123662\n",
      "[LightGBM] [Info] Start training from score -1.245302\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001169 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2313\n",
      "[LightGBM] [Info] Number of data points in the train set: 14087, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -0.362462\n",
      "[LightGBM] [Info] Start training from score -4.123662\n",
      "[LightGBM] [Info] Start training from score -1.245302\n",
      "Stacked model accuracy: 0.9021345671200869\n",
      "0:\tlearn: 1.0295807\ttotal: 8.73ms\tremaining: 8.72s\n",
      "200:\tlearn: 0.2677137\ttotal: 3.63s\tremaining: 14.4s\n",
      "400:\tlearn: 0.2449505\ttotal: 7.17s\tremaining: 10.7s\n",
      "600:\tlearn: 0.2279689\ttotal: 10.6s\tremaining: 7.04s\n",
      "800:\tlearn: 0.2136143\ttotal: 14.2s\tremaining: 3.52s\n",
      "999:\tlearn: 0.2022112\ttotal: 18.4s\tremaining: 0us\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004314 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2326\n",
      "[LightGBM] [Info] Number of data points in the train set: 22009, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -0.362423\n",
      "[LightGBM] [Info] Start training from score -4.124276\n",
      "[LightGBM] [Info] Start training from score -1.245362\n",
      "0:\tlearn: 1.0274453\ttotal: 10.6ms\tremaining: 10.6s\n",
      "200:\tlearn: 0.2422674\ttotal: 3.42s\tremaining: 13.6s\n",
      "400:\tlearn: 0.2182612\ttotal: 6.67s\tremaining: 9.97s\n",
      "600:\tlearn: 0.2013990\ttotal: 10s\tremaining: 6.67s\n",
      "800:\tlearn: 0.1873033\ttotal: 13.4s\tremaining: 3.33s\n",
      "999:\tlearn: 0.1749912\ttotal: 16.7s\tremaining: 0us\n",
      "0:\tlearn: 1.0276269\ttotal: 10.5ms\tremaining: 10.5s\n",
      "200:\tlearn: 0.2477358\ttotal: 3.58s\tremaining: 14.3s\n",
      "400:\tlearn: 0.2242579\ttotal: 6.93s\tremaining: 10.4s\n",
      "600:\tlearn: 0.2071281\ttotal: 10.2s\tremaining: 6.75s\n",
      "800:\tlearn: 0.1921153\ttotal: 14.2s\tremaining: 3.52s\n",
      "999:\tlearn: 0.1798749\ttotal: 18.4s\tremaining: 0us\n",
      "0:\tlearn: 1.0277690\ttotal: 9.71ms\tremaining: 9.7s\n",
      "200:\tlearn: 0.2469893\ttotal: 3.55s\tremaining: 14.1s\n",
      "400:\tlearn: 0.2232241\ttotal: 7.22s\tremaining: 10.8s\n",
      "600:\tlearn: 0.2051184\ttotal: 10.4s\tremaining: 6.92s\n",
      "800:\tlearn: 0.1906699\ttotal: 13.7s\tremaining: 3.4s\n",
      "999:\tlearn: 0.1778518\ttotal: 17.3s\tremaining: 0us\n",
      "0:\tlearn: 1.0313583\ttotal: 16.4ms\tremaining: 16.4s\n",
      "200:\tlearn: 0.2808109\ttotal: 3.25s\tremaining: 12.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400:\tlearn: 0.2550011\ttotal: 6.41s\tremaining: 9.57s\n",
      "600:\tlearn: 0.2362081\ttotal: 9.65s\tremaining: 6.4s\n",
      "800:\tlearn: 0.2201670\ttotal: 13.1s\tremaining: 3.25s\n",
      "999:\tlearn: 0.2066961\ttotal: 16.4s\tremaining: 0us\n",
      "0:\tlearn: 1.0340089\ttotal: 12.9ms\tremaining: 12.9s\n",
      "200:\tlearn: 0.3063576\ttotal: 3.29s\tremaining: 13.1s\n",
      "400:\tlearn: 0.2788275\ttotal: 6.58s\tremaining: 9.82s\n",
      "600:\tlearn: 0.2585538\ttotal: 9.85s\tremaining: 6.54s\n",
      "800:\tlearn: 0.2423294\ttotal: 13.1s\tremaining: 3.25s\n",
      "999:\tlearn: 0.2279014\ttotal: 16.5s\tremaining: 0us\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001122 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2318\n",
      "[LightGBM] [Info] Number of data points in the train set: 17607, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -0.362444\n",
      "[LightGBM] [Info] Start training from score -4.123563\n",
      "[LightGBM] [Info] Start training from score -1.245350\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010765 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2325\n",
      "[LightGBM] [Info] Number of data points in the train set: 17607, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -0.362444\n",
      "[LightGBM] [Info] Start training from score -4.123563\n",
      "[LightGBM] [Info] Start training from score -1.245350\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001376 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2315\n",
      "[LightGBM] [Info] Number of data points in the train set: 17607, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -0.362444\n",
      "[LightGBM] [Info] Start training from score -4.123563\n",
      "[LightGBM] [Info] Start training from score -1.245350\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001035 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2320\n",
      "[LightGBM] [Info] Number of data points in the train set: 17607, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -0.362363\n",
      "[LightGBM] [Info] Start training from score -4.127078\n",
      "[LightGBM] [Info] Start training from score -1.245350\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001459 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2318\n",
      "[LightGBM] [Info] Number of data points in the train set: 17608, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -0.362419\n",
      "[LightGBM] [Info] Start training from score -4.123619\n",
      "[LightGBM] [Info] Start training from score -1.245407\n",
      "Stacking Model Log Loss: 0.1282131042831433\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "\n",
    "# #only given train_data\n",
    "# X_simple_xgb=np.save('X_train_simple_xgb.npy',X_simple_xgb)\n",
    "# train_label=np.save('X_train_label.npy',train_label)\n",
    "\n",
    "#pseudo label 0.8 ver train data\n",
    "\n",
    "X_simple_xgb = np.load('X_pseudo_simple_xgb.npy')\n",
    "train_label=np.load(\"pseudo_train_label.npy\")\n",
    "\n",
    "test_data = pd.read_csv('test.csv')\n",
    "test_id=test_data['id']\n",
    "X_test_simple_xgb=np.load(\"X_test_simple_xgb.npy\")\n",
    "\n",
    "# # SMOTE for handling class imbalance\n",
    "# smote = SMOTE()\n",
    "# X_resampled, y_resampled = smote.fit_resample(X_simple_xgb, train_label)\n",
    "\n",
    "#smote 없이 하기!\n",
    "X_resampled=X_simple_xgb\n",
    "y_resampled=train_label\n",
    "\n",
    "dtrain = xgb.DMatrix(X_resampled, label=y_resampled)\n",
    "dtest = xgb.DMatrix(X_test_simple_xgb)\n",
    "\n",
    "\n",
    "# Model definitions\n",
    "xgb_model = xgb.XGBClassifier(objective='multi:softprob', num_class=3, tree_method='hist', device='cuda', random_state=42)\n",
    "rf = RandomForestClassifier()\n",
    "cat_model = CatBoostClassifier(iterations=1000, learning_rate=0.05, depth=6, loss_function='MultiClass', random_seed=42, verbose=200)\n",
    "lgb_model = lgb.LGBMClassifier(objective='multiclass', num_class=3, random_state=42)\n",
    "\n",
    "# Load the best params from the pickle file\n",
    "with open(\"best_params.pkl\", \"rb\") as file:\n",
    "    best_params = pickle.load(file)\n",
    "\n",
    "# You can now use the loaded parameters\n",
    "print(best_params)\n",
    "\n",
    "best_params = best_params\n",
    "\n",
    "# Ensemble model using stacking\n",
    "estimators = [\n",
    "    ('xgb', xgb.XGBClassifier(**best_params, objective='multi:softprob', num_class=3, tree_method='hist', device='cuda', random_state=42)),\n",
    "    ('catboost', cat_model),\n",
    "    ('lgb', lgb_model)\n",
    "]\n",
    "stack_model = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(max_iter=10000))\n",
    "\n",
    "# Cross-validation\n",
    "stack_scores = cross_val_score(stack_model, X_resampled, y_resampled, cv=5, scoring='accuracy')\n",
    "print(f'Stacked model accuracy: {stack_scores.mean()}') #no Pseudo 이용시\n",
    "\n",
    "#Stacked model accuracy: 0.9379823641166685 이거 나옴..처음에 -->random search는 내가 초반에 이미함. (using pseudo)\n",
    "\n",
    "# Train the stacked model and predict\n",
    "stack_model.fit(X_resampled, y_resampled)\n",
    "test_preds = stack_model.predict_proba(X_test_simple_xgb)\n",
    "print(f\"Stacking Model Log Loss: {log_loss_stack}\")\n",
    "\n",
    "# Export submission\n",
    "test_id = test_data['id']\n",
    "output_df = pd.DataFrame(test_preds, columns=['Status_C', 'Status_CL', 'Status_D'])\n",
    "output_df.insert(0, 'id', test_id)\n",
    "output_df.to_csv('final_submission_no_smote_ensemble.csv', index=False)\n",
    "\n",
    "# CatBoost and LightGBM for additional predictions\n",
    "# cat_model.fit(X_resampled, y_resampled)\n",
    "# lgb_model.fit(X_resampled, y_resampled)\n",
    "\n",
    "# # Predict using CatBoost and LightGBM\n",
    "# cat_preds = cat_model.predict_proba(X_test_simple_xgb)\n",
    "# lgb_preds = lgb_model.predict_proba(X_test_simple_xgb)\n",
    "\n",
    "# # Export CatBoost predictions\n",
    "# output_cat = pd.DataFrame(cat_preds, columns=['Status_C', 'Status_CL', 'Status_D'])\n",
    "# output_cat.insert(0, 'id', test_id)\n",
    "# output_cat.to_csv('final_submission_catboost.csv', index=False)\n",
    "\n",
    "# # Export LightGBM predictions\n",
    "# output_lgb = pd.DataFrame(lgb_preds, columns=['Status_C', 'Status_CL', 'Status_D'])\n",
    "# output_lgb.insert(0, 'id', test_id)\n",
    "# output_lgb.to_csv('final_submission_lightgbm.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88136dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.0509979\ttotal: 32.7ms\tremaining: 32.7s\n",
      "200:\tlearn: 0.2908337\ttotal: 6.83s\tremaining: 27.2s\n",
      "400:\tlearn: 0.2132922\ttotal: 13.4s\tremaining: 20.1s\n",
      "600:\tlearn: 0.1732974\ttotal: 19.7s\tremaining: 13.1s\n",
      "800:\tlearn: 0.1482512\ttotal: 25.6s\tremaining: 6.37s\n",
      "999:\tlearn: 0.1311558\ttotal: 31.5s\tremaining: 0us\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003923 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4590\n",
      "[LightGBM] [Info] Number of data points in the train set: 36763, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -1.098639\n",
      "[LightGBM] [Info] Start training from score -1.098558\n",
      "[LightGBM] [Info] Start training from score -1.098639\n",
      "0:\tlearn: 1.0517277\ttotal: 95.2ms\tremaining: 1m 35s\n",
      "200:\tlearn: 0.2739379\ttotal: 5.97s\tremaining: 23.7s\n",
      "400:\tlearn: 0.1889324\ttotal: 12s\tremaining: 18s\n",
      "600:\tlearn: 0.1450955\ttotal: 18.3s\tremaining: 12.1s\n",
      "800:\tlearn: 0.1186649\ttotal: 25.2s\tremaining: 6.26s\n",
      "999:\tlearn: 0.1008444\ttotal: 32.7s\tremaining: 0us\n",
      "0:\tlearn: 1.0504958\ttotal: 13.2ms\tremaining: 13.2s\n",
      "200:\tlearn: 0.2973680\ttotal: 7.8s\tremaining: 31s\n",
      "400:\tlearn: 0.2159108\ttotal: 15.5s\tremaining: 23.1s\n",
      "600:\tlearn: 0.1769242\ttotal: 22.6s\tremaining: 15s\n",
      "800:\tlearn: 0.1519960\ttotal: 29.8s\tremaining: 7.4s\n",
      "999:\tlearn: 0.1332791\ttotal: 36.5s\tremaining: 0us\n",
      "0:\tlearn: 1.0498233\ttotal: 14.1ms\tremaining: 14.1s\n",
      "200:\tlearn: 0.2874187\ttotal: 7.12s\tremaining: 28.3s\n",
      "400:\tlearn: 0.2104760\ttotal: 14.1s\tremaining: 21.1s\n",
      "600:\tlearn: 0.1725452\ttotal: 21.1s\tremaining: 14s\n",
      "800:\tlearn: 0.1488556\ttotal: 29.1s\tremaining: 7.23s\n",
      "999:\tlearn: 0.1326492\ttotal: 35.8s\tremaining: 0us\n",
      "0:\tlearn: 1.0511167\ttotal: 31ms\tremaining: 31s\n",
      "200:\tlearn: 0.3035732\ttotal: 8.7s\tremaining: 34.6s\n",
      "400:\tlearn: 0.2250926\ttotal: 15.3s\tremaining: 22.9s\n",
      "600:\tlearn: 0.1857667\ttotal: 21.2s\tremaining: 14.1s\n",
      "800:\tlearn: 0.1605765\ttotal: 26.8s\tremaining: 6.66s\n",
      "999:\tlearn: 0.1430228\ttotal: 32.3s\tremaining: 0us\n",
      "0:\tlearn: 1.0517748\ttotal: 13.9ms\tremaining: 13.9s\n",
      "200:\tlearn: 0.3028439\ttotal: 6.24s\tremaining: 24.8s\n",
      "400:\tlearn: 0.2240641\ttotal: 13.3s\tremaining: 19.8s\n",
      "600:\tlearn: 0.1845657\ttotal: 21.1s\tremaining: 14s\n",
      "800:\tlearn: 0.1610397\ttotal: 27s\tremaining: 6.7s\n",
      "999:\tlearn: 0.1429316\ttotal: 33s\tremaining: 0us\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.091860 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4588\n",
      "[LightGBM] [Info] Number of data points in the train set: 29410, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -1.098646\n",
      "[LightGBM] [Info] Start training from score -1.098544\n",
      "[LightGBM] [Info] Start training from score -1.098646\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002974 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4590\n",
      "[LightGBM] [Info] Number of data points in the train set: 29410, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -1.098646\n",
      "[LightGBM] [Info] Start training from score -1.098544\n",
      "[LightGBM] [Info] Start training from score -1.098646\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002887 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4589\n",
      "[LightGBM] [Info] Number of data points in the train set: 29410, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -1.098646\n",
      "[LightGBM] [Info] Start training from score -1.098544\n",
      "[LightGBM] [Info] Start training from score -1.098646\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014568 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4590\n",
      "[LightGBM] [Info] Number of data points in the train set: 29411, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -1.098680\n",
      "[LightGBM] [Info] Start training from score -1.098578\n",
      "[LightGBM] [Info] Start training from score -1.098578\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001806 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4590\n",
      "[LightGBM] [Info] Number of data points in the train set: 29411, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -1.098578\n",
      "[LightGBM] [Info] Start training from score -1.098578\n",
      "[LightGBM] [Info] Start training from score -1.098680\n",
      "0:\tlearn: 1.0525200\ttotal: 16.5ms\tremaining: 16.5s\n",
      "200:\tlearn: 0.3318421\ttotal: 6.72s\tremaining: 26.7s\n",
      "400:\tlearn: 0.2519417\ttotal: 13.6s\tremaining: 20.4s\n",
      "600:\tlearn: 0.2109693\ttotal: 20.3s\tremaining: 13.5s\n",
      "800:\tlearn: 0.1858737\ttotal: 26.8s\tremaining: 6.66s\n",
      "999:\tlearn: 0.1667703\ttotal: 33.1s\tremaining: 0us\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002288 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4588\n",
      "[LightGBM] [Info] Number of data points in the train set: 36763, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -1.098639\n",
      "[LightGBM] [Info] Start training from score -1.098639\n",
      "[LightGBM] [Info] Start training from score -1.098558\n",
      "0:\tlearn: 1.0527125\ttotal: 18.7ms\tremaining: 18.7s\n",
      "200:\tlearn: 0.2850290\ttotal: 6.85s\tremaining: 27.2s\n",
      "400:\tlearn: 0.2031903\ttotal: 13.1s\tremaining: 19.5s\n",
      "600:\tlearn: 0.1617618\ttotal: 19.6s\tremaining: 13s\n",
      "800:\tlearn: 0.1347084\ttotal: 25.9s\tremaining: 6.44s\n",
      "999:\tlearn: 0.1161886\ttotal: 32.4s\tremaining: 0us\n",
      "0:\tlearn: 1.0525058\ttotal: 103ms\tremaining: 1m 42s\n",
      "200:\tlearn: 0.3300842\ttotal: 6.81s\tremaining: 27.1s\n",
      "400:\tlearn: 0.2506674\ttotal: 12.8s\tremaining: 19.1s\n",
      "600:\tlearn: 0.2093542\ttotal: 18.9s\tremaining: 12.6s\n",
      "800:\tlearn: 0.1837592\ttotal: 25.7s\tremaining: 6.38s\n",
      "999:\tlearn: 0.1639196\ttotal: 32.4s\tremaining: 0us\n",
      "0:\tlearn: 1.0524557\ttotal: 16.4ms\tremaining: 16.4s\n",
      "200:\tlearn: 0.3313800\ttotal: 6.28s\tremaining: 25s\n",
      "400:\tlearn: 0.2552763\ttotal: 13.3s\tremaining: 19.9s\n",
      "600:\tlearn: 0.2154247\ttotal: 20s\tremaining: 13.3s\n",
      "800:\tlearn: 0.1889547\ttotal: 26.3s\tremaining: 6.53s\n",
      "999:\tlearn: 0.1695847\ttotal: 32.8s\tremaining: 0us\n",
      "0:\tlearn: 1.0542367\ttotal: 91.2ms\tremaining: 1m 31s\n",
      "200:\tlearn: 0.3481634\ttotal: 6.62s\tremaining: 26.3s\n",
      "400:\tlearn: 0.2708076\ttotal: 12.7s\tremaining: 19s\n",
      "600:\tlearn: 0.2304293\ttotal: 19.1s\tremaining: 12.7s\n",
      "800:\tlearn: 0.2041178\ttotal: 25.7s\tremaining: 6.39s\n",
      "999:\tlearn: 0.1847807\ttotal: 32s\tremaining: 0us\n",
      "0:\tlearn: 1.0546053\ttotal: 15.8ms\tremaining: 15.8s\n",
      "200:\tlearn: 0.3500279\ttotal: 6.41s\tremaining: 25.5s\n",
      "400:\tlearn: 0.2708250\ttotal: 13.2s\tremaining: 19.8s\n",
      "600:\tlearn: 0.2291812\ttotal: 19.6s\tremaining: 13s\n",
      "800:\tlearn: 0.2033831\ttotal: 25.9s\tremaining: 6.43s\n",
      "999:\tlearn: 0.1835265\ttotal: 32.7s\tremaining: 0us\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002285 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4588\n",
      "[LightGBM] [Info] Number of data points in the train set: 29410, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -1.098646\n",
      "[LightGBM] [Info] Start training from score -1.098646\n",
      "[LightGBM] [Info] Start training from score -1.098544\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002817 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4588\n",
      "[LightGBM] [Info] Number of data points in the train set: 29410, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -1.098646\n",
      "[LightGBM] [Info] Start training from score -1.098646\n",
      "[LightGBM] [Info] Start training from score -1.098544\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001907 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4588\n",
      "[LightGBM] [Info] Number of data points in the train set: 29410, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -1.098646\n",
      "[LightGBM] [Info] Start training from score -1.098646\n",
      "[LightGBM] [Info] Start training from score -1.098544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001686 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4589\n",
      "[LightGBM] [Info] Number of data points in the train set: 29411, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -1.098680\n",
      "[LightGBM] [Info] Start training from score -1.098578\n",
      "[LightGBM] [Info] Start training from score -1.098578\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001646 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4588\n",
      "[LightGBM] [Info] Number of data points in the train set: 29411, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -1.098578\n",
      "[LightGBM] [Info] Start training from score -1.098680\n",
      "[LightGBM] [Info] Start training from score -1.098578\n",
      "0:\tlearn: 1.0518812\ttotal: 20.3ms\tremaining: 20.3s\n",
      "200:\tlearn: 0.3301621\ttotal: 6.01s\tremaining: 23.9s\n",
      "400:\tlearn: 0.2566524\ttotal: 12.3s\tremaining: 18.4s\n",
      "600:\tlearn: 0.2198892\ttotal: 18.2s\tremaining: 12.1s\n",
      "800:\tlearn: 0.1963424\ttotal: 24.2s\tremaining: 6s\n",
      "999:\tlearn: 0.1782656\ttotal: 30.1s\tremaining: 0us\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002191 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4590\n",
      "[LightGBM] [Info] Number of data points in the train set: 36763, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -1.098639\n",
      "[LightGBM] [Info] Start training from score -1.098639\n",
      "[LightGBM] [Info] Start training from score -1.098558\n",
      "0:\tlearn: 1.0501682\ttotal: 26.7ms\tremaining: 26.6s\n",
      "200:\tlearn: 0.2947265\ttotal: 5.53s\tremaining: 22s\n",
      "400:\tlearn: 0.2206386\ttotal: 11.2s\tremaining: 16.7s\n",
      "600:\tlearn: 0.1823322\ttotal: 16.8s\tremaining: 11.1s\n",
      "800:\tlearn: 0.1574575\ttotal: 22.1s\tremaining: 5.49s\n",
      "999:\tlearn: 0.1403314\ttotal: 27.9s\tremaining: 0us\n",
      "0:\tlearn: 1.0511882\ttotal: 18.5ms\tremaining: 18.5s\n",
      "200:\tlearn: 0.3195769\ttotal: 5.58s\tremaining: 22.2s\n",
      "400:\tlearn: 0.2425608\ttotal: 11.2s\tremaining: 16.7s\n",
      "600:\tlearn: 0.2027383\ttotal: 16.7s\tremaining: 11.1s\n",
      "800:\tlearn: 0.1773431\ttotal: 22.3s\tremaining: 5.54s\n",
      "999:\tlearn: 0.1593471\ttotal: 27.8s\tremaining: 0us\n",
      "0:\tlearn: 1.0535467\ttotal: 19.7ms\tremaining: 19.7s\n",
      "200:\tlearn: 0.3417178\ttotal: 5.88s\tremaining: 23.4s\n",
      "400:\tlearn: 0.2682428\ttotal: 11.6s\tremaining: 17.3s\n",
      "600:\tlearn: 0.2292487\ttotal: 17.1s\tremaining: 11.4s\n",
      "800:\tlearn: 0.2042741\ttotal: 22.9s\tremaining: 5.69s\n",
      "999:\tlearn: 0.1854822\ttotal: 28.4s\tremaining: 0us\n",
      "0:\tlearn: 1.0528052\ttotal: 23.1ms\tremaining: 23.1s\n",
      "200:\tlearn: 0.3473161\ttotal: 5.71s\tremaining: 22.7s\n",
      "400:\tlearn: 0.2768749\ttotal: 11.6s\tremaining: 17.3s\n",
      "600:\tlearn: 0.2400151\ttotal: 16.8s\tremaining: 11.2s\n",
      "800:\tlearn: 0.2153321\ttotal: 22.1s\tremaining: 5.5s\n",
      "999:\tlearn: 0.1964024\ttotal: 27.3s\tremaining: 0us\n",
      "0:\tlearn: 1.0536513\ttotal: 15.7ms\tremaining: 15.7s\n",
      "200:\tlearn: 0.3471767\ttotal: 5.8s\tremaining: 23s\n",
      "400:\tlearn: 0.2755297\ttotal: 11.9s\tremaining: 17.7s\n",
      "600:\tlearn: 0.2395743\ttotal: 18.9s\tremaining: 12.6s\n",
      "800:\tlearn: 0.2151759\ttotal: 25.8s\tremaining: 6.41s\n",
      "999:\tlearn: 0.1964182\ttotal: 32.1s\tremaining: 0us\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002682 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4589\n",
      "[LightGBM] [Info] Number of data points in the train set: 29410, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -1.098646\n",
      "[LightGBM] [Info] Start training from score -1.098646\n",
      "[LightGBM] [Info] Start training from score -1.098544\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001320 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4589\n",
      "[LightGBM] [Info] Number of data points in the train set: 29410, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -1.098646\n",
      "[LightGBM] [Info] Start training from score -1.098646\n",
      "[LightGBM] [Info] Start training from score -1.098544\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013624 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4588\n",
      "[LightGBM] [Info] Number of data points in the train set: 29410, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -1.098646\n",
      "[LightGBM] [Info] Start training from score -1.098646\n",
      "[LightGBM] [Info] Start training from score -1.098544\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001235 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4588\n",
      "[LightGBM] [Info] Number of data points in the train set: 29411, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -1.098680\n",
      "[LightGBM] [Info] Start training from score -1.098578\n",
      "[LightGBM] [Info] Start training from score -1.098578\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001930 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4587\n",
      "[LightGBM] [Info] Number of data points in the train set: 29411, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -1.098578\n",
      "[LightGBM] [Info] Start training from score -1.098680\n",
      "[LightGBM] [Info] Start training from score -1.098578\n",
      "0:\tlearn: 1.0528252\ttotal: 19.8ms\tremaining: 19.8s\n",
      "200:\tlearn: 0.3390919\ttotal: 7.62s\tremaining: 30.3s\n",
      "400:\tlearn: 0.2647243\ttotal: 15.9s\tremaining: 23.8s\n",
      "600:\tlearn: 0.2285576\ttotal: 22.2s\tremaining: 14.8s\n",
      "800:\tlearn: 0.2047301\ttotal: 29.2s\tremaining: 7.26s\n",
      "999:\tlearn: 0.1873764\ttotal: 35.6s\tremaining: 0us\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011493 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4590\n",
      "[LightGBM] [Info] Number of data points in the train set: 36763, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -1.098558\n",
      "[LightGBM] [Info] Start training from score -1.098639\n",
      "[LightGBM] [Info] Start training from score -1.098639\n",
      "0:\tlearn: 1.0512443\ttotal: 19.1ms\tremaining: 19s\n",
      "200:\tlearn: 0.3056971\ttotal: 5.69s\tremaining: 22.6s\n",
      "400:\tlearn: 0.2304167\ttotal: 11s\tremaining: 16.4s\n",
      "600:\tlearn: 0.1913827\ttotal: 16.6s\tremaining: 11s\n",
      "800:\tlearn: 0.1669386\ttotal: 22s\tremaining: 5.47s\n",
      "999:\tlearn: 0.1500774\ttotal: 27.3s\tremaining: 0us\n",
      "0:\tlearn: 1.0522208\ttotal: 16.7ms\tremaining: 16.7s\n",
      "200:\tlearn: 0.3298644\ttotal: 5.6s\tremaining: 22.3s\n",
      "400:\tlearn: 0.2544766\ttotal: 11s\tremaining: 16.4s\n",
      "600:\tlearn: 0.2146368\ttotal: 16.4s\tremaining: 10.9s\n",
      "800:\tlearn: 0.1888846\ttotal: 21.9s\tremaining: 5.44s\n",
      "999:\tlearn: 0.1704822\ttotal: 27.3s\tremaining: 0us\n",
      "0:\tlearn: 1.0537557\ttotal: 20.5ms\tremaining: 20.5s\n",
      "200:\tlearn: 0.3546456\ttotal: 5.46s\tremaining: 21.7s\n",
      "400:\tlearn: 0.2810449\ttotal: 11s\tremaining: 16.4s\n",
      "600:\tlearn: 0.2431708\ttotal: 16.4s\tremaining: 10.9s\n",
      "800:\tlearn: 0.2162605\ttotal: 22s\tremaining: 5.45s\n",
      "999:\tlearn: 0.1970685\ttotal: 27.7s\tremaining: 0us\n",
      "0:\tlearn: 1.0529084\ttotal: 22.5ms\tremaining: 22.5s\n",
      "200:\tlearn: 0.3476779\ttotal: 5.67s\tremaining: 22.5s\n",
      "400:\tlearn: 0.2750232\ttotal: 11.3s\tremaining: 16.8s\n",
      "600:\tlearn: 0.2389145\ttotal: 16.7s\tremaining: 11.1s\n",
      "800:\tlearn: 0.2150314\ttotal: 22.4s\tremaining: 5.58s\n",
      "999:\tlearn: 0.1973283\ttotal: 28.2s\tremaining: 0us\n",
      "0:\tlearn: 1.0547697\ttotal: 22.9ms\tremaining: 22.9s\n",
      "200:\tlearn: 0.3574374\ttotal: 6.71s\tremaining: 26.7s\n",
      "400:\tlearn: 0.2855339\ttotal: 13.5s\tremaining: 20.2s\n",
      "600:\tlearn: 0.2488167\ttotal: 20.1s\tremaining: 13.4s\n",
      "800:\tlearn: 0.2249084\ttotal: 25.5s\tremaining: 6.33s\n",
      "999:\tlearn: 0.2061844\ttotal: 31.8s\tremaining: 0us\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007795 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4590\n",
      "[LightGBM] [Info] Number of data points in the train set: 29410, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -1.098544\n",
      "[LightGBM] [Info] Start training from score -1.098646\n",
      "[LightGBM] [Info] Start training from score -1.098646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018304 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4589\n",
      "[LightGBM] [Info] Number of data points in the train set: 29410, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -1.098544\n",
      "[LightGBM] [Info] Start training from score -1.098646\n",
      "[LightGBM] [Info] Start training from score -1.098646\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001718 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4589\n",
      "[LightGBM] [Info] Number of data points in the train set: 29410, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -1.098544\n",
      "[LightGBM] [Info] Start training from score -1.098646\n",
      "[LightGBM] [Info] Start training from score -1.098646\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012084 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4588\n",
      "[LightGBM] [Info] Number of data points in the train set: 29411, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -1.098578\n",
      "[LightGBM] [Info] Start training from score -1.098578\n",
      "[LightGBM] [Info] Start training from score -1.098680\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004674 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4589\n",
      "[LightGBM] [Info] Number of data points in the train set: 29411, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -1.098578\n",
      "[LightGBM] [Info] Start training from score -1.098680\n",
      "[LightGBM] [Info] Start training from score -1.098578\n",
      "0:\tlearn: 1.0539266\ttotal: 20.5ms\tremaining: 20.5s\n",
      "200:\tlearn: 0.3456762\ttotal: 6.46s\tremaining: 25.7s\n",
      "400:\tlearn: 0.2724805\ttotal: 12.9s\tremaining: 19.2s\n",
      "600:\tlearn: 0.2353168\ttotal: 19.4s\tremaining: 12.9s\n",
      "800:\tlearn: 0.2103557\ttotal: 25.7s\tremaining: 6.38s\n",
      "999:\tlearn: 0.1930034\ttotal: 31.7s\tremaining: 0us\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002534 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4590\n",
      "[LightGBM] [Info] Number of data points in the train set: 36764, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -1.098585\n",
      "[LightGBM] [Info] Start training from score -1.098585\n",
      "[LightGBM] [Info] Start training from score -1.098667\n",
      "0:\tlearn: 1.0523913\ttotal: 92.9ms\tremaining: 1m 32s\n",
      "200:\tlearn: 0.3128441\ttotal: 7.54s\tremaining: 30s\n",
      "400:\tlearn: 0.2357876\ttotal: 14.7s\tremaining: 22s\n",
      "600:\tlearn: 0.1974613\ttotal: 22.5s\tremaining: 14.9s\n",
      "800:\tlearn: 0.1729501\ttotal: 29.1s\tremaining: 7.22s\n",
      "999:\tlearn: 0.1547183\ttotal: 37.2s\tremaining: 0us\n",
      "0:\tlearn: 1.0534455\ttotal: 17.8ms\tremaining: 17.7s\n",
      "200:\tlearn: 0.3386902\ttotal: 5.73s\tremaining: 22.8s\n",
      "400:\tlearn: 0.2596206\ttotal: 11.7s\tremaining: 17.5s\n",
      "600:\tlearn: 0.2206290\ttotal: 17.3s\tremaining: 11.5s\n",
      "800:\tlearn: 0.1956639\ttotal: 23.1s\tremaining: 5.74s\n",
      "999:\tlearn: 0.1768679\ttotal: 29.2s\tremaining: 0us\n",
      "0:\tlearn: 1.0550637\ttotal: 12.2ms\tremaining: 12.2s\n",
      "200:\tlearn: 0.3652559\ttotal: 5.76s\tremaining: 22.9s\n",
      "400:\tlearn: 0.2889106\ttotal: 12.1s\tremaining: 18.1s\n",
      "600:\tlearn: 0.2508959\ttotal: 18.4s\tremaining: 12.2s\n",
      "800:\tlearn: 0.2246908\ttotal: 24.3s\tremaining: 6.03s\n",
      "999:\tlearn: 0.2046457\ttotal: 30.3s\tremaining: 0us\n",
      "0:\tlearn: 1.0535919\ttotal: 15.9ms\tremaining: 15.8s\n",
      "200:\tlearn: 0.3522423\ttotal: 5.47s\tremaining: 21.7s\n",
      "400:\tlearn: 0.2798707\ttotal: 10.8s\tremaining: 16.2s\n",
      "600:\tlearn: 0.2438183\ttotal: 16.7s\tremaining: 11.1s\n",
      "800:\tlearn: 0.2202048\ttotal: 22.2s\tremaining: 5.52s\n",
      "999:\tlearn: 0.2020009\ttotal: 27.5s\tremaining: 0us\n",
      "0:\tlearn: 1.0559664\ttotal: 22.1ms\tremaining: 22.1s\n",
      "200:\tlearn: 0.3638103\ttotal: 5.48s\tremaining: 21.8s\n",
      "400:\tlearn: 0.2916168\ttotal: 11.2s\tremaining: 16.8s\n",
      "600:\tlearn: 0.2540015\ttotal: 17.1s\tremaining: 11.3s\n",
      "800:\tlearn: 0.2298109\ttotal: 22.5s\tremaining: 5.58s\n",
      "999:\tlearn: 0.2111994\ttotal: 28s\tremaining: 0us\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002792 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4590\n",
      "[LightGBM] [Info] Number of data points in the train set: 29411, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -1.098578\n",
      "[LightGBM] [Info] Start training from score -1.098578\n",
      "[LightGBM] [Info] Start training from score -1.098680\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004470 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4588\n",
      "[LightGBM] [Info] Number of data points in the train set: 29411, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -1.098578\n",
      "[LightGBM] [Info] Start training from score -1.098578\n",
      "[LightGBM] [Info] Start training from score -1.098680\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002420 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4588\n",
      "[LightGBM] [Info] Number of data points in the train set: 29411, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -1.098578\n",
      "[LightGBM] [Info] Start training from score -1.098578\n",
      "[LightGBM] [Info] Start training from score -1.098680\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002472 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4590\n",
      "[LightGBM] [Info] Number of data points in the train set: 29411, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -1.098578\n",
      "[LightGBM] [Info] Start training from score -1.098578\n",
      "[LightGBM] [Info] Start training from score -1.098680\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001646 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4589\n",
      "[LightGBM] [Info] Number of data points in the train set: 29412, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "Stacked model log loss: -0.20138806630843162\n",
      "0:\tlearn: 1.0524417\ttotal: 44.8ms\tremaining: 44.7s\n",
      "200:\tlearn: 0.3281655\ttotal: 5.84s\tremaining: 23.2s\n",
      "400:\tlearn: 0.2528054\ttotal: 11.8s\tremaining: 17.6s\n",
      "600:\tlearn: 0.2156570\ttotal: 17.6s\tremaining: 11.7s\n",
      "800:\tlearn: 0.1906151\ttotal: 23s\tremaining: 5.7s\n",
      "999:\tlearn: 0.1735391\ttotal: 28s\tremaining: 0us\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002388 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4590\n",
      "[LightGBM] [Info] Number of data points in the train set: 45954, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "0:\tlearn: 1.0509979\ttotal: 37.4ms\tremaining: 37.3s\n",
      "200:\tlearn: 0.2908337\ttotal: 5s\tremaining: 19.9s\n",
      "400:\tlearn: 0.2132922\ttotal: 10.4s\tremaining: 15.5s\n",
      "600:\tlearn: 0.1732974\ttotal: 15.8s\tremaining: 10.5s\n",
      "800:\tlearn: 0.1482512\ttotal: 21.3s\tremaining: 5.29s\n",
      "999:\tlearn: 0.1311558\ttotal: 26.5s\tremaining: 0us\n",
      "0:\tlearn: 1.0525200\ttotal: 36.5ms\tremaining: 36.4s\n",
      "200:\tlearn: 0.3318421\ttotal: 5.32s\tremaining: 21.2s\n",
      "400:\tlearn: 0.2519417\ttotal: 10.9s\tremaining: 16.3s\n",
      "600:\tlearn: 0.2109693\ttotal: 16.3s\tremaining: 10.8s\n",
      "800:\tlearn: 0.1858737\ttotal: 22s\tremaining: 5.46s\n",
      "999:\tlearn: 0.1667703\ttotal: 27.2s\tremaining: 0us\n",
      "0:\tlearn: 1.0518812\ttotal: 65.2ms\tremaining: 1m 5s\n",
      "200:\tlearn: 0.3301621\ttotal: 5.7s\tremaining: 22.6s\n",
      "400:\tlearn: 0.2566524\ttotal: 11.4s\tremaining: 17s\n",
      "600:\tlearn: 0.2198892\ttotal: 17.5s\tremaining: 11.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800:\tlearn: 0.1963424\ttotal: 23.5s\tremaining: 5.83s\n",
      "999:\tlearn: 0.1782656\ttotal: 29.1s\tremaining: 0us\n",
      "0:\tlearn: 1.0528252\ttotal: 44.4ms\tremaining: 44.4s\n",
      "200:\tlearn: 0.3390919\ttotal: 5.81s\tremaining: 23.1s\n",
      "400:\tlearn: 0.2647243\ttotal: 11.2s\tremaining: 16.8s\n",
      "600:\tlearn: 0.2285576\ttotal: 16.9s\tremaining: 11.2s\n",
      "800:\tlearn: 0.2047301\ttotal: 22.5s\tremaining: 5.6s\n",
      "999:\tlearn: 0.1873764\ttotal: 28.2s\tremaining: 0us\n",
      "0:\tlearn: 1.0539266\ttotal: 39.1ms\tremaining: 39s\n",
      "200:\tlearn: 0.3456762\ttotal: 5.98s\tremaining: 23.8s\n",
      "400:\tlearn: 0.2724805\ttotal: 11.7s\tremaining: 17.5s\n",
      "600:\tlearn: 0.2353168\ttotal: 17.7s\tremaining: 11.7s\n",
      "800:\tlearn: 0.2103557\ttotal: 23.1s\tremaining: 5.73s\n",
      "999:\tlearn: 0.1930034\ttotal: 28.8s\tremaining: 0us\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007410 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4590\n",
      "[LightGBM] [Info] Number of data points in the train set: 36763, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -1.098639\n",
      "[LightGBM] [Info] Start training from score -1.098558\n",
      "[LightGBM] [Info] Start training from score -1.098639\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006163 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4588\n",
      "[LightGBM] [Info] Number of data points in the train set: 36763, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -1.098639\n",
      "[LightGBM] [Info] Start training from score -1.098639\n",
      "[LightGBM] [Info] Start training from score -1.098558\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003812 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4590\n",
      "[LightGBM] [Info] Number of data points in the train set: 36763, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -1.098639\n",
      "[LightGBM] [Info] Start training from score -1.098639\n",
      "[LightGBM] [Info] Start training from score -1.098558\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007447 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4590\n",
      "[LightGBM] [Info] Number of data points in the train set: 36763, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -1.098558\n",
      "[LightGBM] [Info] Start training from score -1.098639\n",
      "[LightGBM] [Info] Start training from score -1.098639\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006371 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4590\n",
      "[LightGBM] [Info] Number of data points in the train set: 36764, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -1.098585\n",
      "[LightGBM] [Info] Start training from score -1.098585\n",
      "[LightGBM] [Info] Start training from score -1.098667\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'random_search_xgb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 57\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Log Loss for different models\u001b[39;00m\n\u001b[0;32m     56\u001b[0m log_loss_stack \u001b[38;5;241m=\u001b[39m log_loss(y_resampled, stack_model\u001b[38;5;241m.\u001b[39mpredict_proba(X_resampled))\n\u001b[1;32m---> 57\u001b[0m log_loss_xgb \u001b[38;5;241m=\u001b[39m log_loss(y_resampled, \u001b[43mrandom_search_xgb\u001b[49m\u001b[38;5;241m.\u001b[39mpredict_proba(X_resampled))\n\u001b[0;32m     58\u001b[0m log_loss_catboost \u001b[38;5;241m=\u001b[39m log_loss(y_resampled, cat_model\u001b[38;5;241m.\u001b[39mpredict_proba(X_resampled))\n\u001b[0;32m     59\u001b[0m log_loss_lgb \u001b[38;5;241m=\u001b[39m log_loss(y_resampled, lgb_model\u001b[38;5;241m.\u001b[39mpredict_proba(X_resampled))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'random_search_xgb' is not defined"
     ]
    }
   ],
   "source": [
    "# 방법 2:\n",
    "\n",
    "# #only given train_data\n",
    "# X_simple_xgb=np.save('X_train_simple_xgb.npy',X_simple_xgb)\n",
    "# train_label=np.save('X_train_label.npy',train_label)\n",
    "\n",
    "#pseudo label 0.8 ver train data\n",
    "\n",
    "X_simple_xgb = np.load('X_pseudo_simple_xgb.npy')\n",
    "train_label=np.load(\"pseudo_train_label.npy\")\n",
    "\n",
    "\n",
    "X_test_simple_xgb = np.load('X_test_simple_xgb.npy')\n",
    "\n",
    "# SMOTE for handling class imbalance\n",
    "smote = SMOTE()\n",
    "X_resampled, y_resampled = smote.fit_resample(X_simple_xgb, train_label)\n",
    "\n",
    "#put gpu\n",
    "dtrain = xgb.DMatrix(X_resampled, label=y_resampled)\n",
    "dtest = xgb.DMatrix(X_test_simple_xgb)\n",
    "\n",
    "# Hyperparameter Optimization for XGBoost\n",
    "\n",
    "\n",
    "# Support Vector Machine (SVM)\n",
    "svm_model = SVC(probability=True, kernel='rbf', C=1, gamma='scale')\n",
    "\n",
    "# CatBoost Model\n",
    "cat_model = CatBoostClassifier(iterations=1000, learning_rate=0.05, depth=6, loss_function='MultiClass', random_seed=42, verbose=200)\n",
    "\n",
    "# LightGBM Model\n",
    "lgb_model = lgb.LGBMClassifier(objective='multiclass', num_class=3, random_state=42)\n",
    "\n",
    "# Ensemble with Meta-Learning (Stacking) Including SVM\n",
    "estimators = [\n",
    "    ('xgb', xgb.XGBClassifier(**best_params, objective='multi:softprob', num_class=3, tree_method='hist', device='cuda', random_state=42)),\n",
    "    ('catboost', cat_model),\n",
    "    ('lgb', lgb_model),\n",
    "    ('svm', svm_model)\n",
    "]\n",
    "\n",
    "# Final Estimator with Logistic Regression (Meta-Learning)\n",
    "stack_model = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(max_iter=10000))\n",
    "\n",
    "# Cross-validation for Log Loss\n",
    "stack_scores = cross_val_score(stack_model, X_resampled, y_resampled, cv=5, scoring='neg_log_loss')\n",
    "print(f'Stacked model log loss: {stack_scores.mean()}')\n",
    "\n",
    "\n",
    "# Train the final stacking model\n",
    "stack_model.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Predict with the Stacking Model\n",
    "test_preds = stack_model.predict_proba(X_test_simple_xgb)\n",
    "\n",
    "# Export submission\n",
    "test_id = pd.read_csv('test.csv')['id']\n",
    "output_df = pd.DataFrame(test_preds, columns=['Status_C', 'Status_CL', 'Status_D'])\n",
    "output_df.insert(0, 'id', test_id)\n",
    "output_df.to_csv('final_submission_ensemble_with_svm.csv', index=False)\n",
    "\n",
    "# Log Loss for different models\n",
    "log_loss_stack = log_loss(y_resampled, stack_model.predict_proba(X_resampled))\n",
    "# log_loss_xgb = log_loss(y_resampled, random_search_xgb.predict_proba(X_resampled))\n",
    "log_loss_catboost = log_loss(y_resampled, cat_model.predict_proba(X_resampled))\n",
    "log_loss_lgb = log_loss(y_resampled, lgb_model.predict_proba(X_resampled))\n",
    "\n",
    "print(f\"Stacking Model Log Loss: {log_loss_stack}\")\n",
    "# print(f\"XGBoost Model Log Loss: {log_loss_xgb}\")\n",
    "print(f\"CatBoost Model Log Loss: {log_loss_catboost}\")\n",
    "print(f\"LightGBM Model Log Loss: {log_loss_lgb}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5bc88b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9186241991912399\n"
     ]
    }
   ],
   "source": [
    "train_score = stack_model.score(X_simple_xgb, train_label) #- SVM 추가 시 결과!\n",
    "print(train_score) #0.9239856422372665 (svm추가, pseudo data, smote 이용시)\n",
    "# no svm, pseudo data , smote 이용 x 0.9186241991912399"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7cd4e54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully.\n",
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "#save svm+ 모델\n",
    "\n",
    "# Save the trained model\n",
    "# with open('final_svm_ensemble_stack_model.pkl', 'wb') as f:\n",
    "#     pickle.dump(stack_model, f)\n",
    "# print(\"Model saved successfully.\")\n",
    "\n",
    "#load\n",
    "# Load the saved model\n",
    "with open('final_svm_ensemble_stack_model.pkl', 'rb') as f:\n",
    "    loaded_stack_model = pickle.load(f)\n",
    "print(\"Model loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e30676f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StackingClassifier(estimators=[(&#x27;xgb&#x27;,\n",
       "                                XGBClassifier(base_score=None, booster=None,\n",
       "                                              callbacks=None,\n",
       "                                              colsample_bylevel=None,\n",
       "                                              colsample_bynode=None,\n",
       "                                              colsample_bytree=0.8,\n",
       "                                              device=&#x27;cuda&#x27;,\n",
       "                                              early_stopping_rounds=None,\n",
       "                                              enable_categorical=False,\n",
       "                                              eval_metric=None,\n",
       "                                              feature_types=None, gamma=1,\n",
       "                                              grow_policy=None,\n",
       "                                              importance_type=None,\n",
       "                                              interaction_constraints=None,\n",
       "                                              learning_ra...\n",
       "                                              monotone_constraints=None,\n",
       "                                              multi_strategy=None,\n",
       "                                              n_estimators=300, n_jobs=None,\n",
       "                                              num_class=3,\n",
       "                                              num_parallel_tree=None, ...)),\n",
       "                               (&#x27;catboost&#x27;,\n",
       "                                &lt;catboost.core.CatBoostClassifier object at 0x0000010AF6D353A0&gt;),\n",
       "                               (&#x27;lgb&#x27;,\n",
       "                                LGBMClassifier(num_class=3,\n",
       "                                               objective=&#x27;multiclass&#x27;,\n",
       "                                               random_state=42)),\n",
       "                               (&#x27;svm&#x27;, SVC(C=1, probability=True))],\n",
       "                   final_estimator=LogisticRegression(max_iter=10000))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;StackingClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.StackingClassifier.html\">?<span>Documentation for StackingClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>StackingClassifier(estimators=[(&#x27;xgb&#x27;,\n",
       "                                XGBClassifier(base_score=None, booster=None,\n",
       "                                              callbacks=None,\n",
       "                                              colsample_bylevel=None,\n",
       "                                              colsample_bynode=None,\n",
       "                                              colsample_bytree=0.8,\n",
       "                                              device=&#x27;cuda&#x27;,\n",
       "                                              early_stopping_rounds=None,\n",
       "                                              enable_categorical=False,\n",
       "                                              eval_metric=None,\n",
       "                                              feature_types=None, gamma=1,\n",
       "                                              grow_policy=None,\n",
       "                                              importance_type=None,\n",
       "                                              interaction_constraints=None,\n",
       "                                              learning_ra...\n",
       "                                              monotone_constraints=None,\n",
       "                                              multi_strategy=None,\n",
       "                                              n_estimators=300, n_jobs=None,\n",
       "                                              num_class=3,\n",
       "                                              num_parallel_tree=None, ...)),\n",
       "                               (&#x27;catboost&#x27;,\n",
       "                                &lt;catboost.core.CatBoostClassifier object at 0x0000010AF6D353A0&gt;),\n",
       "                               (&#x27;lgb&#x27;,\n",
       "                                LGBMClassifier(num_class=3,\n",
       "                                               objective=&#x27;multiclass&#x27;,\n",
       "                                               random_state=42)),\n",
       "                               (&#x27;svm&#x27;, SVC(C=1, probability=True))],\n",
       "                   final_estimator=LogisticRegression(max_iter=10000))</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>xgb</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">XGBClassifier</label><div class=\"sk-toggleable__content fitted\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.8, device=&#x27;cuda&#x27;, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=1, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=3, max_leaves=None,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=300, n_jobs=None, num_class=3,\n",
       "              num_parallel_tree=None, ...)</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>catboost</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">CatBoostClassifier</label><div class=\"sk-toggleable__content fitted\"><pre>&lt;catboost.core.CatBoostClassifier object at 0x0000010AF6D353A0&gt;</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>lgb</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">LGBMClassifier</label><div class=\"sk-toggleable__content fitted\"><pre>LGBMClassifier(num_class=3, objective=&#x27;multiclass&#x27;, random_state=42)</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>svm</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;SVC<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.svm.SVC.html\">?<span>Documentation for SVC</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>SVC(C=1, probability=True)</pre></div> </div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>final_estimator</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(max_iter=10000)</pre></div> </div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "StackingClassifier(estimators=[('xgb',\n",
       "                                XGBClassifier(base_score=None, booster=None,\n",
       "                                              callbacks=None,\n",
       "                                              colsample_bylevel=None,\n",
       "                                              colsample_bynode=None,\n",
       "                                              colsample_bytree=0.8,\n",
       "                                              device='cuda',\n",
       "                                              early_stopping_rounds=None,\n",
       "                                              enable_categorical=False,\n",
       "                                              eval_metric=None,\n",
       "                                              feature_types=None, gamma=1,\n",
       "                                              grow_policy=None,\n",
       "                                              importance_type=None,\n",
       "                                              interaction_constraints=None,\n",
       "                                              learning_ra...\n",
       "                                              monotone_constraints=None,\n",
       "                                              multi_strategy=None,\n",
       "                                              n_estimators=300, n_jobs=None,\n",
       "                                              num_class=3,\n",
       "                                              num_parallel_tree=None, ...)),\n",
       "                               ('catboost',\n",
       "                                <catboost.core.CatBoostClassifier object at 0x0000010AF6D353A0>),\n",
       "                               ('lgb',\n",
       "                                LGBMClassifier(num_class=3,\n",
       "                                               objective='multiclass',\n",
       "                                               random_state=42)),\n",
       "                               ('svm', SVC(C=1, probability=True))],\n",
       "                   final_estimator=LogisticRegression(max_iter=10000))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_model\n",
    "lgb_model\n",
    "stack_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2116c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Model Log Loss: 0.1282131042831433\n"
     ]
    }
   ],
   "source": [
    "# Log Loss for different models\n",
    "log_loss_stack = log_loss(y_resampled, stack_model.predict_proba(X_resampled))\n",
    "# log_loss_xgb = log_loss(y_resampled, random_search_xgb.predict_proba(X_resampled))\n",
    "# log_loss_catboost = log_loss(y_resampled, cat_model.predict_proba(X_resampled))\n",
    "# log_loss_lgb = log_loss(y_resampled, lgb_model.predict_proba(X_resampled))\n",
    "\n",
    "print(f\"Stacking Model Log Loss: {log_loss_stack}\") #--> #Stacking Model Log Loss: 0.1282131042831433 (svm꺼)\n",
    "#final tuning 이후 확인 필요\n",
    "# print(f\"CatBoost Model Log Loss: {log_loss_catboost}\")\n",
    "# print(f\"LightGBM Model Log Loss: {log_loss_lgb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96e60e98",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_pytorchenv\\lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_pytorchenv\\lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 43\u001b[0m\n\u001b[0;32m     33\u001b[0m final_estimator_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[0;32m     34\u001b[0m     stack_model, \u001b[38;5;66;03m#기존의 stack model의 base model은 그대로 사용!ㅏ\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     40\u001b[0m )\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m \u001b[43mfinal_estimator_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_resampled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_resampled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Print the results for the final estimator\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m({\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogistic_regression\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_score\u001b[39m\u001b[38;5;124m'\u001b[39m: final_estimator_search\u001b[38;5;241m.\u001b[39mbest_score_,\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_params\u001b[39m\u001b[38;5;124m'\u001b[39m: final_estimator_search\u001b[38;5;241m.\u001b[39mbest_params_\n\u001b[0;32m     50\u001b[0m })\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_pytorchenv\\lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_pytorchenv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1019\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1014\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1015\u001b[0m     )\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1019\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1023\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_pytorchenv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1573\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1572\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1573\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_pytorchenv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:965\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    960\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    961\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    962\u001b[0m         )\n\u001b[0;32m    963\u001b[0m     )\n\u001b[1;32m--> 965\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    984\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    987\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    988\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_pytorchenv\\lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_pytorchenv\\lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_pytorchenv\\lib\\site-packages\\joblib\\parallel.py:1703\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1701\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[0;32m   1702\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m-> 1703\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_abort\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1704\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1706\u001b[0m     \u001b[38;5;66;03m# Store the unconsumed tasks and terminate the workers if necessary\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_pytorchenv\\lib\\site-packages\\joblib\\parallel.py:1614\u001b[0m, in \u001b[0;36mParallel._abort\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1609\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborted \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabort_everything\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m   1610\u001b[0m     \u001b[38;5;66;03m# If the backend is managed externally we need to make sure\u001b[39;00m\n\u001b[0;32m   1611\u001b[0m     \u001b[38;5;66;03m# to leave it in a working state to allow for future jobs\u001b[39;00m\n\u001b[0;32m   1612\u001b[0m     \u001b[38;5;66;03m# scheduling.\u001b[39;00m\n\u001b[0;32m   1613\u001b[0m     ensure_ready \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_managed_backend\n\u001b[1;32m-> 1614\u001b[0m     \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabort_everything\u001b[49m\u001b[43m(\u001b[49m\u001b[43mensure_ready\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_ready\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1615\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_pytorchenv\\lib\\site-packages\\joblib\\_parallel_backends.py:620\u001b[0m, in \u001b[0;36mLokyBackend.abort_everything\u001b[1;34m(self, ensure_ready)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mabort_everything\u001b[39m(\u001b[38;5;28mself\u001b[39m, ensure_ready\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    618\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Shutdown the workers and restart a new one with the same parameters\u001b[39;00m\n\u001b[0;32m    619\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_workers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mterminate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkill_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    621\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ensure_ready:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_pytorchenv\\lib\\site-packages\\joblib\\executor.py:75\u001b[0m, in \u001b[0;36mMemmappingExecutor.terminate\u001b[1;34m(self, kill_workers)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mterminate\u001b[39m(\u001b[38;5;28mself\u001b[39m, kill_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 75\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkill_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkill_workers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;66;03m# When workers are killed in a brutal manner, they cannot execute the\u001b[39;00m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;66;03m# finalizer of their shared memmaps. The refcount of those memmaps may\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;66;03m# be off by an unknown number, so instead of decref'ing them, we force\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;66;03m# with allow_non_empty=True but if we can't, it will be clean up later\u001b[39;00m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;66;03m# on by the resource_tracker.\u001b[39;00m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_submit_resize_lock:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_pytorchenv\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:1303\u001b[0m, in \u001b[0;36mProcessPoolExecutor.shutdown\u001b[1;34m(self, wait, kill_workers)\u001b[0m\n\u001b[0;32m   1299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m executor_manager_thread \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m wait:\n\u001b[0;32m   1300\u001b[0m     \u001b[38;5;66;03m# This locks avoids concurrent join if the interpreter\u001b[39;00m\n\u001b[0;32m   1301\u001b[0m     \u001b[38;5;66;03m# is shutting down.\u001b[39;00m\n\u001b[0;32m   1302\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _global_shutdown_lock:\n\u001b[1;32m-> 1303\u001b[0m         \u001b[43mexecutor_manager_thread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1304\u001b[0m         _threads_wakeups\u001b[38;5;241m.\u001b[39mpop(executor_manager_thread, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;66;03m# To reduce the risk of opening too many files, remove references to\u001b[39;00m\n\u001b[0;32m   1307\u001b[0m \u001b[38;5;66;03m# objects that use file descriptors.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_pytorchenv\\lib\\threading.py:1060\u001b[0m, in \u001b[0;36mThread.join\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1057\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1060\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1062\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[0;32m   1063\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_pytorchenv\\lib\\threading.py:1080\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m   1077\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1079\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1080\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1081\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m   1082\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#find best parameter for stacked model.\n",
    "\n",
    "# Base classifiers (your previous models: RandomForest, SVM, XGBoost, DecisionTree)\n",
    "# Assuming `rf`, `svm`, `xg`, and `dt` are already defined and hyperparameters are optimized\n",
    "# Example:\n",
    "\n",
    "# \n",
    "#기존 pre-trained 된 모델들 불러오기!\n",
    "\n",
    "# # Create the StackingClassifier with the base classifiers\n",
    "# estimator_list = [\n",
    "#     ('svm', svm),\n",
    "#     ('xgb', xgb_model),\n",
    "#     ('catboost', cat_model),\n",
    "#     ('lgb',lgb_model)\n",
    "    \n",
    "# ]\n",
    "\n",
    "# stack_model = StackingClassifier(\n",
    "#     estimators=estimator_list,\n",
    "#     final_estimator=LogisticRegression(max_iter=100000)\n",
    "# )\n",
    "\n",
    "# Define hyperparameter grid for the final estimator (Logistic Regression)\n",
    "final_estimator_params = {\n",
    "    'final_estimator__penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    'final_estimator__C': np.logspace(-4, 4, 20),\n",
    "    'final_estimator__solver': ['lbfgs', 'newton-cg', 'liblinear', 'sag', 'saga'],\n",
    "    'final_estimator__max_iter': [50, 100, 250, 500]\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV to optimize the final estimator in the stacking model\n",
    "final_estimator_search = GridSearchCV(\n",
    "    stack_model, #기존의 stack model의 base model은 그대로 사용!ㅏ\n",
    "    \n",
    "    param_grid=final_estimator_params,\n",
    "    cv=5,\n",
    "    return_train_score=False,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "final_estimator_search.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Print the results for the final estimator\n",
    "print({\n",
    "    'model': 'logistic_regression',\n",
    "    'best_score': final_estimator_search.best_score_,\n",
    "    'best_params': final_estimator_search.best_params_\n",
    "})\n",
    "\n",
    "# Predict with the best stacking model\n",
    "test_preds = final_estimator_search.best_estimator_.predict_proba(X_test_simple_xgb)\n",
    "\n",
    "# Export the results\n",
    "test_id = test_data['id']\n",
    "output_df = pd.DataFrame(test_preds, columns=['Status_C', 'Status_CL', 'Status_D'])\n",
    "output_df.insert(0, 'id', test_id)\n",
    "output_df.to_csv('final_best_param_submission_stacking.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gpu_pytorchenv)",
   "language": "python",
   "name": "gpu_pytorchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
